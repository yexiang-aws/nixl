---
#
# Key Components:
# - Job Configuration: Defines timeout, failure behavior, and server resources
# - Docker Images: Specifies the container images used for different build stages
# - Matrix Axes: Defines build variations (currently x86_64 architecture)
# - Run Steps: Sequential steps for running tests
#
# When Modified:
# - Adding/removing Docker images: Affects available test environments
# - Modifying matrix axes: Changes test variations (e.g., adding architectures)
# - Adjusting resource limits: Impacts test performance and resource allocation
# - Adding/removing steps: Changes the test pipeline sequence
#
# Note: Changes to this file are tested as part of the PR CI flow no need to test them manually.


job: nixl-ci-gpu

# Fail job if one of the steps fails or continue
failFast: false

timeout_minutes: 240

registry_host: harbor.mellanox.com
registry_auth: nixl_harbor_credentials
registry_path: /nixl/ci

env:
  NIXL_INSTALL_DIR: /opt/nixl
  TEST_TIMEOUT: 30
  NPROC: 32
  CI_IMAGE_TAG: "20251218-1"

docker_opt: "-e NPROC -e EXECUTOR_NUMBER --security-opt seccomp=unconfined --security-opt apparmor=unconfined --shm-size=1G --ulimit nofile=65535:65535 --ulimit stack=67108864 --ulimit memlock=-1:-1 --network=host --ipc=host --cap-add=SYS_PTRACE --cap-add=SYS_ADMIN --privileged --gpus all --device=/dev/infiniband --device=/dev/gdrdrv --entrypoint=''"

# label is defined at jenkins slave configuration, we want to run the job on a gpu agent and be able to esaly replace it without having to change this file
runs_on_dockers:
  - {
     file: '.ci/dockerfiles/Dockerfile.ci', name: 'nixl-ci-test-25.10-cuda13.0-ubuntu24.04', tag: "${CI_IMAGE_TAG}",
     build_args: '--build-arg NIXL_INSTALL_DIR=${NIXL_INSTALL_DIR}  --build-arg BASE_IMAGE=nvcr.io/nvidia/cuda-dl-base:25.10-cuda13.0-devel-ubuntu24.04 --build-arg PRE_INSTALLED_UCX_ENV=true --build-arg PRE_INSTALLED_NIXL_ENV=true --build-arg ARCH=${arch} --pull --no-cache',
     nodeLabel: 'H100'
    }
  #- {
  #   file: '.ci/dockerfiles/Dockerfile.ci', name: 'nixl-ci-test1-25.10-cuda13.0-ubuntu24.04', tag: "${CI_IMAGE_TAG}",
  #   build_args: '--build-arg NIXL_INSTALL_DIR=${NIXL_INSTALL_DIR}  --build-arg BASE_IMAGE=nvcr.io/nvidia/cuda-dl-base:25.10-cuda13.0-devel-ubuntu24.04 --build-arg PRE_INSTALLED_UCX_ENV=true --build-arg PRE_INSTALLED_NIXL_ENV=true --build-arg ARCH=${arch} --pull --no-cache',
  #   nodeLabel: 'DGX'
  #  }

matrix:
  axes:
    arch:
      - x86_64
    ucx_version:
      - master
      - v1.20.x

taskName: "${name}/${arch}/ucx-${ucx_version}/${axis_index}"

steps:
  - name: Get Environment Info
    parallel: false
    run: |
      set -x
      set +e
      # print kernel version
      uname -r
      # print nvidia drivers info
      lsmod | grep nvidia_peermem
      lsmod | grep gdrdrv
      lsmod | grep nvidia_fs
      # print nvidia-smi
      nvidia-smi
      nvidia-smi topo -m
      # print compute mode
      nvidia-smi -q | grep -i "compute mode"
      # check rdma status
      ibv_devinfo
      #ib_write_bw
      # Check and configure GPU compute mode
      # Exclusive_Process mode is required for MPS workloads
      GPU_COUNT=$(nvidia-smi -L | wc -l)
      if [ "$GPU_COUNT" -gt 0 ]; then
        for i in $(seq 0 $((GPU_COUNT-1))); do
          # Query current compute mode for each GPU
          COMPUTE_MODE=$(nvidia-smi -i $i --query-gpu=compute_mode --format=csv,noheader)
          echo "GPU $i compute mode: $COMPUTE_MODE"

          # Set to EXCLUSIVE_PROCESS mode if not already set
          if [ "$COMPUTE_MODE" != "Exclusive_Process" ]; then
            echo "Setting GPU $i to EXCLUSIVE_PROCESS mode"
            sudo nvidia-smi -i $i -c EXCLUSIVE_PROCESS || echo "Warning: Failed to set compute mode for GPU $i"
          else
            echo "GPU $i is already in EXCLUSIVE_PROCESS mode"
          fi
        done
      fi

  - name: Build
    parallel: false
    run: |
      export UCX_VERSION=${ucx_version}
      export PRE_INSTALLED_ENV="true"
      export NIXL_BUILD_DIR=nixl_build_${ucx_version}
      export NIXLBENCH_BUILD_DIR=nixlbench_build_${ucx_version}
      .gitlab/build.sh ${NIXL_INSTALL_DIR}

  - name: Test CPP
    parallel: false
    timeout: "${TEST_TIMEOUT}"
    run: |
      .gitlab/test_cpp.sh ${NIXL_INSTALL_DIR}

  - name: Test Python
    parallel: false
    timeout: "${TEST_TIMEOUT}"
    run: |
      .gitlab/test_python.sh ${NIXL_INSTALL_DIR}

  - name: Test Nixlbench
    parallel: false
    timeout: "${TEST_TIMEOUT}"
    run: |
      .gitlab/test_nixlbench.sh ${NIXL_INSTALL_DIR}

  - name: Test Rust
    parallel: false
    timeout: "${TEST_TIMEOUT}"
    run: |
      .gitlab/test_rust.sh ${NIXL_INSTALL_DIR}
